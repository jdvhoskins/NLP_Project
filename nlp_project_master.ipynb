{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 583701 words in the corpus.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import BracketParseCorpusReader\n",
    "from nltk import *\n",
    "import re\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        corpus.words(): list of str\n",
    "        corpus.sents(): list of (list of str)\n",
    "\n",
    "        corpus.tagged_words(): list of (str,str) tuple\n",
    "        corpus.tagged_sents(): list of (list of (str,str))\n",
    "        \n",
    "        corpus.chunked_sents(): list of (Tree w/ (str,str) leaves)\n",
    "        corpus.parsed_sents(): list of (Tree with str leaves)\n",
    "        \n",
    "        corpus.xml(): A single xml ElementTree\n",
    "        corpus.raw(): str (unprocessed corpus contents)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Import the corpus\n",
    "corpus_root = './corpus/'\n",
    "corpus = BracketParseCorpusReader(corpus_root, \".*\")\n",
    "\n",
    "\n",
    "tagged_sents = corpus.tagged_sents()\n",
    "ngram_input = []\n",
    "\n",
    "\n",
    "#  Remove -NONE- tags from the training data and create a list of tokens with no tags.\n",
    "for sent in tagged_sents:\n",
    "    for token in sent:\n",
    "        if token[1] == \"-NONE-\" or token[1] == \"SYM\":\n",
    "            del token\n",
    "        else:\n",
    "            ngram_input.append(token[0].lower())\n",
    "#            ngram_input.append(token[0])\n",
    "    ngram_input.append(\"\")\n",
    "\n",
    "print(\"There are\",len(ngram_input),\"words in the corpus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583700\n",
      "583699\n",
      "29586\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create bigram and trigram lists\n",
    "bgram_raw = list(ngrams(ngram_input,2))\n",
    "tgram_raw = list(ngrams(ngram_input,3))\n",
    "\n",
    "bgram=list(bgram_raw)\n",
    "tgram=list(tgram_raw)\n",
    "\n",
    "print(len(bgram))\n",
    "print(len(tgram))\n",
    "print(len(set(ngram_input)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= weight than capital of her mother and what will help . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "def get_next(matrix, seed1, seed2=none):\n",
    "#    num = random.randint()\n",
    "\n",
    "    if seed2 == none:\n",
    "        \n",
    "    else:\n",
    "        print(2)\n",
    "\n",
    "\n",
    "Use this as a bae for building cpd_t\n",
    "\n",
    "cfd_t = FreqDist()\n",
    "for word_pair in SOMETHING:\n",
    "    fdist[word_pair] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def generate_bg_sentence(dist, word):\n",
    "    while True:\n",
    "        print(word, end=' ')\n",
    "        word = dist[word].generate()\n",
    "        if word == \"\":\n",
    "            break\n",
    "\n",
    "def get_next(dist, token):\n",
    "    return dist[token].generate()\n",
    "\n",
    "cfd_b = ConditionalFreqDist(bgram_raw)\n",
    "cpd_b = ConditionalProbDist(cfd_b, MLEProbDist)\n",
    "\n",
    "\n",
    "\n",
    "#cfd_t = ConditionalFreqDist(ngrams(ngram_input,3))\n",
    "#cpd_t = ConditionalProbDist(cfd_b, MLEProbDist)\n",
    "\n",
    "\n",
    "seed = \"=\"\n",
    "\n",
    "#print(cfd_b[seed].tabulate())\n",
    "\n",
    "#for word in cpd_b[seed].samples():\n",
    "#    print(cpd_b[seed].prob(word))\n",
    "\n",
    "#print(cpd_b['time'].generate())\n",
    "for i in range(1):\n",
    "    generate_bg_sentence(cpd_b, seed)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serendipity' 'mine' 'divergent' 'pancreatic' 'naively' 'capturado'\n",
      " 'lighted' 'threatened' 'derelicts' 'wrecks' 'nancy' 'generalizations'\n",
      " 'stoutness' 'florally' 'hillings' 'with' 'motivations' 't0' 'antony'\n",
      " 'splinters']\n",
      "\n",
      "serendipity at the \t-23.7178471351\n",
      "\n",
      "mine who made their resources remain when all are n't really scary ; a world and turned and depreciation on the other allies your own laws banning african - even told us who has moved in the revised charter defines most of its tusk ?. \t-192.057344186\n",
      "\n",
      "divergent that particular items given signal , to \" \t-47.0730421502\n",
      "\n",
      "pancreatic islets and international style meal . \" so do violence -- like a market puts chips . \t-76.7623018487\n",
      "\n",
      "naively driven by : belief , his hive . \t-42.5486073339\n",
      "\n",
      "capturado that he had become bloody american bird areas , deep recession can earn the first detected in a crush of money available credit and detailed causal connection between june 2001 . \t-136.942706761\n",
      "\n",
      "lighted windows , states , â€œ you are usually in a job offer the wine and can and developers remove this copy falls into the contract 3 3$$4 years . \t-136.239139404\n",
      "\n",
      "threatened to make that you needed . \t-34.1747314951\n",
      "\n",
      "derelicts and through a jar of chemical information called him very important that 's man asked once yuck , rather the dover area , but wo n't you first time ? \t-139.536773611\n",
      "\n",
      "wrecks were said before emil ! \t-34.4049714422\n",
      "\n",
      "nancy , sony ones you have infiltrated and eve told me to contribute to cut your cell is our troops . \t-99.1564712392\n",
      "\n",
      "generalizations : none \t-17.308528513\n",
      "\n",
      "stoutness and special project - net income gained a number of life like you have a youngster to remember the same as tiffany , gives me to -- his 39th birthday to the ship ? \t-152.858843749\n",
      "\n",
      "florally patterned chair for me to support to process for that happens to achieve true that 's not to stop beginning of which in response . \t-109.313368873\n",
      "\n",
      "hillings 09$$08$$99 11:57 am mrs ernest hemingway \t-22.7620813493\n",
      "\n",
      "with the new york times , which mies van park has cut . \t-45.426404198\n",
      "\n",
      "motivations , most attracting product testimonials from experiments at a big - rail transport network of the fact , later she 's much bigger than hf-nl -lrb- accession w59173 -rrb- that meeting i pulled into a diplomat is , will turner : as possible significant and world think that an apartment / \t-222.017721864\n",
      "\n",
      "t0 iur list !? \t-17.8097436376\n",
      "\n",
      "antony . \t-13.3527845647\n",
      "\n",
      "splinters and then that a vampire tv screen with your gift to begin with livefree patches , it could never said that lenders were the comments that 's happy birthday anniversary card . \t-137.772590583\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(43)\n",
    "\n",
    "#temp = set(ngram_input)\n",
    "unique_tokens = []\n",
    "\n",
    "for string in list(set(ngram_input)):\n",
    "    if string[0:1].isalpha():\n",
    "        unique_tokens.append(string)\n",
    "\n",
    "\n",
    "seed_list = np.random.choice(unique_tokens,20)\n",
    "\n",
    "print(seed_list)\n",
    "\n",
    "def make_sent(dist, seed, tgram=False):\n",
    "    i=0\n",
    "    sent = []\n",
    "    sent.append(seed)\n",
    "    if(not tgram):\n",
    "        while(sent[i]!=\"\"):\n",
    "            sent.append(get_next(dist, sent[i]))\n",
    "            i += 1\n",
    "    else:\n",
    "        sent.append(get_next(dist,sent[i]))\n",
    "        while(sent[i]!=\"\"):\n",
    "            sent.append(get_next(dist,(sent[i-1],sent[i]) ))\n",
    "            i += 1\n",
    "            \n",
    "    return sent\n",
    "\n",
    "\n",
    "def get_prob(dist, ngram_input, sentence):\n",
    "    prob = np.log(ngram_input.count(sentence[0])/len(ngram_input))\n",
    "    \n",
    "    for i in range(1,len(sentence)):\n",
    "        prob += np.log(dist[sentence[i-1]].prob(sentence[i]))\n",
    "\n",
    "    return prob\n",
    "\n",
    "sentences = []\n",
    "full_sents = []\n",
    "prob = []\n",
    "for i in range(len(seed_list)):\n",
    "    sentences.append(make_sent(cpd_b, seed_list[i], False))\n",
    "    prob.append(get_prob(cpd_b, ngram_input, sentences[i]))\n",
    "    full_sents.append(\" \".join(sentences[i]))\n",
    "\n",
    "for i in range(len(full_sents)):\n",
    "    print(\"\\n\" + full_sents[i] + \"\\t\"+ str(prob[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make function to get sentence probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
