{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Generation Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import BracketParseCorpusReader\n",
    "from nltk import *\n",
    "import re\n",
    "\n",
    "\n",
    "# Constants\n",
    "PER_SEED = 30\n",
    "NUM_SEED = 10\n",
    "\n",
    "\n",
    "# Define the functions to be used\n",
    "\n",
    "def get_next(distributions, token, order):\n",
    "    dist = distributions[order-1]\n",
    "    while ( len(list(dist[token].samples())) == 0):\n",
    "        order -= 1\n",
    "        if(order<0 or order>2):\n",
    "            print(\"\\nERROR: \",token,\"HAS NO DISTRIBUTION!!!\\n\")\n",
    "        dist = distributions[order-1]\n",
    "        if (order==1):\n",
    "            token = token[1]\n",
    "        elif (order==2):\n",
    "            token = (token[1], token[2])\n",
    "            \n",
    "    return dist[token].generate()\n",
    "\n",
    "\n",
    "def make_ngram_sentence(distributions, seed, order=1):\n",
    "    dist_b = distributions[0]\n",
    "    dist_t = distributions[1]\n",
    "    dist_f = distributions[2]\n",
    "    i=0\n",
    "    sentence = []\n",
    "    sentence.append(seed)\n",
    "\n",
    "    root = sentence[i]\n",
    "    word1 = get_next(distributions, root, 1)\n",
    "    word2 = get_next(distributions, root, 1)\n",
    "    i += 1\n",
    "\n",
    "    if ( dist_b[root].prob(word1) > dist_b[root].prob(word2) ):\n",
    "        sentence.append(word1)\n",
    "    else:\n",
    "        sentence.append(word2)\n",
    "\n",
    "    if(order == 1):\n",
    "        while(sentence[i] != \"\" or len(sentence) <= 2 ):\n",
    "            root = sentence[i]\n",
    "            word1 = get_next(distributions, root, order)\n",
    "            word2 = get_next(distributions, root, order)\n",
    "            i += 1\n",
    "        \n",
    "            if ( dist_b[root].prob(word1) > dist_b[root].prob(word2) ):\n",
    "                sentence.append(word1)\n",
    "            else:\n",
    "                sentence.append(word2)\n",
    "            \n",
    "        return sentence\n",
    "    \n",
    "    root = (sentence[i-1], sentence[i])\n",
    "    word1 = get_next(distributions, root, 2)\n",
    "    word2 = get_next(distributions, root, 2)\n",
    "    i += 1\n",
    "\n",
    "    if ( dist_t[root].prob(word1) > dist_t[root].prob(word2) ):\n",
    "        sentence.append(word1)\n",
    "    else:\n",
    "        sentence.append(word2)\n",
    "\n",
    "    if(order == 2):\n",
    "        while(sentence[i] != \"\" or len(sentence) <= 3):\n",
    "            root = (sentence[i-1], sentence[i])\n",
    "            word1 = get_next(distributions, root, order)\n",
    "            word2 = get_next(distributions, root, order)\n",
    "            i += 1\n",
    "\n",
    "            if ( dist_t[root].prob(word1) > dist_t[root].prob(word2) ):\n",
    "                sentence.append(word1)\n",
    "            else:\n",
    "                sentence.append(word2)\n",
    "            \n",
    "        return sentence\n",
    "\n",
    "    root = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "    word1 = get_next(distributions, root, 3)\n",
    "    word2 = get_next(distributions, root, 3)\n",
    "    i += 1\n",
    "\n",
    "    if ( dist_f[root].prob(word1) > dist_f[root].prob(word2) ):\n",
    "        sentence.append(word1)\n",
    "    else:\n",
    "        sentence.append(word2)\n",
    "\n",
    "    if(order == 3):\n",
    "        while(sentence[i] != \"\" or len(sentence) <= 4):\n",
    "            root = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "            word1 = get_next(distributions, root, order)\n",
    "            word2 = get_next(distributions, root, order)\n",
    "            i += 1\n",
    "\n",
    "            if ( dist_f[root].prob(word1) > dist_f[root].prob(word2) ):\n",
    "                sentence.append(word1)\n",
    "            else:\n",
    "                sentence.append(word2)\n",
    "            \n",
    "        return sentence\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# Edit this one\n",
    "def make_pos_sentence(dist, seed, is_tgram=False):\n",
    "    i=0\n",
    "    sentence = []\n",
    "    sentence.append(seed)\n",
    "    if(not is_tgram):\n",
    "        while(sentence[i] != \"\"):\n",
    "            sentence.append(get_next(dist, sentence[i]))\n",
    "            i += 1\n",
    "    else:\n",
    "        sentence.append(get_next(dist,sentence[i]))\n",
    "        while(sentence[i] != \"\"):\n",
    "            sentence.append(get_next(dist,(sentence[i-1],sentence[i]) ))\n",
    "            i += 1\n",
    "            \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_ngram_probability(distributions, ngram_input, sentence, order=1):\n",
    "    dist_b = distributions[0]\n",
    "    dist_t = distributions[1]\n",
    "    dist_f = distributions[2]\n",
    "\n",
    "    prob = np.log(ngram_input.count(sentence[0])/len(ngram_input))\n",
    "    prob += np.log( dist_b[sentence[0]].prob(sentence[1]) )\n",
    "\n",
    "    if(order == 1):\n",
    "        for i in range(2,len(sentence)):\n",
    "            condition = sentence[i-1]\n",
    "            prob += np.log(dist_b[condition].prob(sentence[i]))\n",
    "            \n",
    "        return prob/len(sentence)\n",
    "    \n",
    "    condition = (sentence[0], sentence[1])\n",
    "    prob += np.log( dist_t[condition].prob(sentence[2]) )\n",
    "\n",
    "    if(order == 2):\n",
    "        for i in range(3,len(sentence)):\n",
    "            condition = (sentence[i-2], sentence[i-1])\n",
    "            prob += np.log(dist_t[condition].prob(sentence[i]))\n",
    "            \n",
    "        return prob/len(sentence)\n",
    "\n",
    "    condition = (sentence[0], sentence[1], sentence[2])\n",
    "    prob += np.log( dist_f[condition].prob(sentence[3]) )\n",
    "\n",
    "    if(order == 3):\n",
    "        for i in range(4,len(sentence)):\n",
    "            condition = (sentence[i-3], sentence[i-2], sentence[i-1])\n",
    "            prob += np.log(dist_f[condition].prob(sentence[i]))\n",
    "            \n",
    "        return prob/len(sentence)\n",
    "\n",
    "def get_pos_probability(dist, pos_input, tag_sentence):\n",
    "    prob = np.log(pos_input.count(tag_sentence[0][0])/len(pos_input))\n",
    "    \n",
    "    for i in range(1,len(tag_sentence)):\n",
    "        prob += np.log(dist[tag_sentence[i-1]].prob(sentence[i]))\n",
    "\n",
    "    return prob/len(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 543356 tokens in the corpus.\n",
      "There are 25836 unique tokens that start with a letter.\n",
      "There are 44 unique tags in the corpus.\n"
     ]
    }
   ],
   "source": [
    "# Import and parse the corpus\n",
    "\n",
    "corpus_root = './corpus_clean/'\n",
    "corpus = BracketParseCorpusReader(corpus_root, \".*\")\n",
    "\n",
    "tagged_sentences = corpus.tagged_sents()\n",
    "ngram_input = []\n",
    "pos_input = []\n",
    "legal_tags = [\"EOS\",\"$\",\"#\", \"GW\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\",\"JJR\",\"JJS\",\"LS\",\"MD\",\n",
    "             \"NN\",\"NNS\",\"NNP\",'NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO', \"UH\",'VB',\n",
    "             'VBD',\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\", \"\\\"\", \"\\'\", \",\", \".\", \"AFX\"]\n",
    "\n",
    "single_letter_words = [\"a\", \"i\", \",\", \".\", \"!\", \"?\", \"\\'\", \"\\\"\", \":\", ';', '0', '1', '2', \"3\", '4',\n",
    "                       '5', \"6\", '7', '8', \"9\", \"=\", \"&\", \"#\", '/', '>', \"$\", '<', '+', '%',]\n",
    "\n",
    "# tags_removed = [\"-NONE-\",\"SYM\", \"CODE\", \"ADD\", \"HYPH\",\"-LSB-\", \"-RSB-\",\":\", \"NFP\", \"XX\", \"-LRB-\", \"-RRB-\"]\n",
    "\n",
    "#  Remove -NONE- and  SYM tags from the training data and create a list of tokens and a list of tags.\n",
    "for sentence in tagged_sentences:\n",
    "    for token in sentence:\n",
    "        word = token[0].lower()\n",
    "        tag = token[1]\n",
    "        \n",
    "        if(tag == \"NP\"):\n",
    "            tag = \"NNS\"\n",
    "\n",
    "        if not tag in legal_tags:\n",
    "            del token\n",
    "            continue\n",
    "        \n",
    "        if len(word) == 1:\n",
    "            if not word in single_letter_words:\n",
    "                del token\n",
    "                continue\n",
    "        \n",
    "        if (word[0:5] == \"rsquo\"):\n",
    "            word = \"\\'\" + word[5:]\n",
    "\n",
    "        ngram_input.append(word)\n",
    "        pos_input.append(tag)\n",
    "\n",
    "    ngram_input.append(\"\")\n",
    "    pos_input.append(\"EOS\")\n",
    "\n",
    "unique_alphas = []\n",
    "\n",
    "for string in list(set(ngram_input)):\n",
    "    if string[0:1].isalpha():\n",
    "        unique_alphas.append(string)\n",
    "\n",
    "print(\"There are\",len(ngram_input),\"tokens in the corpus.\")\n",
    "print(\"There are\",len(unique_alphas),\"unique tokens that start with a letter.\")\n",
    "\n",
    "\n",
    "tag_set = set(pos_input)\n",
    "print(\"There are\",len(tag_set),\"unique tags in the corpus.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"http:$$$$demo.businesslayers.com\" in ngram_input)\n",
    "print(\"ADD\" in pos_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 543355 bigrams.\n",
      "There are 543354 trigrams.\n",
      "There are 543353 fourgrams.\n"
     ]
    }
   ],
   "source": [
    "# Create bigram and trigram lists\n",
    "bgram = list(ngrams(ngram_input,2))\n",
    "tgram = list(ngrams(ngram_input,3))\n",
    "fgram = list(ngrams(ngram_input,4))\n",
    "\n",
    "pos_bgram = list(ngrams(pos_input,2))\n",
    "\n",
    "\n",
    "# Create conditional frequency distributions\n",
    "cfd_b = ConditionalFreqDist(bgram)\n",
    "\n",
    "cfd_t = ConditionalFreqDist()\n",
    "for trigram in tgram:\n",
    "    condition = (trigram[0], trigram[1])\n",
    "    cfd_t[condition][trigram[2]] += 1\n",
    "\n",
    "cfd_f = ConditionalFreqDist()\n",
    "for fourgram in fgram:\n",
    "    condition = (fourgram[0], fourgram[1], fourgram[2])\n",
    "    cfd_f[condition][fourgram[3]] += 1\n",
    "\n",
    "cfd_pos = ConditionalFreqDist(pos_bgram)\n",
    "\n",
    "cfd_t2w = ConditionalFreqDist()\n",
    "for tag, word in zip(pos_input, ngram_input):\n",
    "    cfd_t2w[tag][word] += 1\n",
    "\n",
    "cfd_w2t = ConditionalFreqDist()\n",
    "for tag, word in zip(pos_input, ngram_input):\n",
    "    cfd_w2t[word][tag] += 1\n",
    "\n",
    "# Create conditional probability distributions\n",
    "cpd_b = ConditionalProbDist(cfd_b, MLEProbDist)\n",
    "cpd_t = ConditionalProbDist(cfd_t, MLEProbDist)\n",
    "cpd_f = ConditionalProbDist(cfd_f, MLEProbDist)\n",
    "\n",
    "cpd_pos = ConditionalProbDist(cfd_pos, MLEProbDist)\n",
    "cpd_t2w = ConditionalProbDist(cfd_t2w, MLEProbDist)\n",
    "cpd_w2t = ConditionalProbDist(cfd_w2t, MLEProbDist)\n",
    "\n",
    "\n",
    "# Consolidate the ngram probability distributions into a single object\n",
    "distributions = [cpd_b, cpd_t, cpd_f]\n",
    "\n",
    "\n",
    "\n",
    "print(\"There are\",len(bgram),\"bigrams.\")\n",
    "print(\"There are\",len(tgram),\"trigrams.\")\n",
    "print(\"There are\",len(fgram),\"fourgrams.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['celli' 'urgently' 'positivity' 'shriek' 'ferried' 'pantyhose' 'sadly'\n",
      " 'coughed' 'systems' 'da-de-da-de-da-de-dat-dat-dah'] \n",
      "\n",
      "\n",
      "celli and basses  example 4 transitions from c♯ to f♯ , and adds pitch class to the pentatonic set of example 3 , outlines a pentatonic set and strongly presents c♯ as the tonal center for the section .  \n",
      " -0.494018898592 \n",
      "\n",
      "urgently furnish this office with your detailed information to enable us process your payment are as follows ; 1 your full names 2 contact address 3 phone numbers 4 qualification educational level 5 age :  \n",
      " -0.455099273094 \n",
      "\n",
      "positivity for ama .  \n",
      " -2.64110400021 \n",
      "\n",
      "shriek pitifully , breath coming in shallow sobs .  \n",
      " -1.3205520001 \n",
      "\n",
      "ferried home to houseboats , and the second greatest terrorist threat comes from several countries , mainly israel , who sometimes resort to terrorism themselves , claiming \" self defense , \" as some israeli neocons would try to say ? \" first year florida coach ron zook said with a sly tone .  \n",
      " -0.436942488405 \n",
      "\n",
      "pantyhose hanging in the bathroom . \"  \n",
      " -1.94461190727 \n",
      "\n",
      "sadly shook its head and says , \" thank god , thank god , thank god , thank god , thank god , thank god , thank god , thank god , thank god , thank god , thank god \" and the cohead of citigroup 's investment bank said he spent \" a small fraction of 1 % \" of his former deputy , anwar ibrahim , who is wielding a fire poker .  \n",
      " -0.41531300648 \n",
      "\n",
      "coughed , i laughed , and then i check , you know , is looking for the phrase , \" separation of church and state ha-ha , she ca n't recall the 14th and 16th amendments and needs to be in the libraries of all who are interested in my ability , and thus exhibit increased commitment and persistence will achieve more of their goals than people with low self efficacy , and new york .  \n",
      " -0.557525015378 \n",
      "\n",
      "systems are irrelevant for instance , polish is written today using the roman alphabet , but russian , a related slavic language , uses the cyrillic ; yiddish , a germanic language , is written in hebrew characters ; latin , greek , and sanskrit , which resemble one another rather closely in some respects , it may be that your sole purpose in life is simply to serve as a warning to others .  \n",
      " -0.300259528944 \n",
      "\n",
      "da-de-da-de-da-de-dat-dat-dah ! \"  she took a sip of her drink , plain water because , she had a glass of morningafter and three cups of coffee , and to get deeper , antie will take a single 360 scan of the headlines found cheek by jowl on the wp ' page a6 provides an interesting snapshot of the life of a loving animal , too .  \n",
      " -0.547925282792 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed_list = np.random.choice(unique_alphas,NUM_SEED)\n",
    "\n",
    "print(seed_list,\"\\n\\n\")\n",
    "\n",
    "for i in range(NUM_SEED):\n",
    "\n",
    "    sentences = []\n",
    "    full_sentences = []\n",
    "    prob = []\n",
    "\n",
    "    for j in range(PER_SEED):\n",
    "        sentences.append(make_ngram_sentence(distributions, seed_list[i], 3))\n",
    "        prob.append(get_ngram_probability(distributions, ngram_input, sentences[j], 3))\n",
    "        full_sentences.append(\" \".join(sentences[j]))\n",
    "\n",
    "    best_index = np.argmax(prob)\n",
    "    print(full_sentences[best_index],\"\\n\",prob[best_index],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['farming', 'everyone', 'good', 'oneself', 'the', 'network', ';', 'and', 'one', 'tourists', 'with', 'my', 'regulations', 'of', 'forces', 'heard', 'up', 'for', 'confident', 'of', 'mirror', 'held', 'the', 'simple', 'victim', 'and', 'one', '18', 'options', ',', 'long', 'to', 'be', 'at', 'senator', '\"', 'when', 'them', ',', 'according', 'myself', 'use', 'benefit', ',', 'building', 'exposition', 'into', 'your', 'lack', 'to', 'do', 'him', ',', 'you', \"'re\", 'up', ',', 'mg', 'in', 'this', 'netscape', 'but', ',', 'the', 'bars', 'including', 'over', 'the', 'time', 'engaged', 'that', 'the', 'has', 'ago', 'of', 'i', 'with', 'me', \"'s\", 'at', 'alongside', 'homes', 'as', 'the', 'laws', 'as', 'gen.', 'enron', 'new', 'ian', ',', 'indicadores', 'hall', 'chung', \"'s\", 'abandoning', ',', 'be', 'a', 'downplayed', 'facility', ',', 'the', 'farmer', 'of', 'their', 'respect', 'to', \"n't\", '.', '']\n"
     ]
    }
   ],
   "source": [
    "def get_next_tag(pos_dist, tag):\n",
    "    return pos_dist[tag].generate()\n",
    "\n",
    "def get_next_word(t2w_dist, tag):\n",
    "    return t2w_dist[tag].generate()\n",
    "\n",
    "\n",
    "def make_pos_sentence(pos_dist, t2w_dist, w2t_dist, seed):\n",
    "    i=0\n",
    "    tag = w2t_dist[seed].generate()\n",
    "\n",
    "    sentence = []\n",
    "    sentence.append(seed)\n",
    "\n",
    "    tags = []\n",
    "    tags.append(tag)\n",
    "\n",
    "    while(sentence[i] != \"\" or len(sentence) <= 2 ):\n",
    "        tags.append(get_next_tag(pos_dist, tags[i]))\n",
    "        sentence.append(get_next_word(t2w_dist, tags[i]))\n",
    "        i += 1\n",
    "        \n",
    "    return sentence\n",
    "    \n",
    "print(make_pos_sentence(cpd_pos, cpd_t2w, cpd_w2t, \"farming\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
