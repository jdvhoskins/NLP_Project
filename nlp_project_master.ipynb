{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Generation Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import BracketParseCorpusReader\n",
    "from nltk import *\n",
    "import re\n",
    "\n",
    "\n",
    "# Constants\n",
    "PER_SEED = 30\n",
    "NUM_SEED = 10\n",
    "\n",
    "\n",
    "# Define the functions to be used\n",
    "\n",
    "def get_next(distributions, token, order):\n",
    "    dist = distributions[order-1]\n",
    "    while ( len(list(dist[token].samples())) == 0):\n",
    "        order -= 1\n",
    "        if(order<0 or order>2):\n",
    "            print(\"\\nERROR: \",token,\"HAS NO DISTRIBUTION!!!\\n\")\n",
    "        dist = distributions[order-1]\n",
    "        if (order==1):\n",
    "            token = token[1]\n",
    "        elif (order==2):\n",
    "            token = (token[1], token[2])\n",
    "            \n",
    "    return dist[token].generate()\n",
    "\n",
    "\n",
    "def make_ngram_sentence(distributions, seed, order=1):\n",
    "    dist_b = distributions[0]\n",
    "    dist_t = distributions[1]\n",
    "    dist_f = distributions[2]\n",
    "    i=0\n",
    "    sentence = []\n",
    "    sentence.append(seed)\n",
    "\n",
    "    root = sentence[i]\n",
    "    word1 = get_next(distributions, root, 1)\n",
    "    word2 = get_next(distributions, root, 1)\n",
    "    i += 1\n",
    "\n",
    "    if ( dist_b[root].prob(word1) > dist_b[root].prob(word2) ):\n",
    "        sentence.append(word1)\n",
    "    else:\n",
    "        sentence.append(word2)\n",
    "\n",
    "    if(order == 1):\n",
    "        while(sentence[i] != \"\" or len(sentence) <= 2 ):\n",
    "            root = sentence[i]\n",
    "            word1 = get_next(distributions, root, order)\n",
    "            word2 = get_next(distributions, root, order)\n",
    "            i += 1\n",
    "        \n",
    "            if ( dist_b[root].prob(word1) > dist_b[root].prob(word2) ):\n",
    "                sentence.append(word1)\n",
    "            else:\n",
    "                sentence.append(word2)\n",
    "            \n",
    "        return sentence\n",
    "    \n",
    "    root = (sentence[i-1], sentence[i])\n",
    "    word1 = get_next(distributions, root, 2)\n",
    "    word2 = get_next(distributions, root, 2)\n",
    "    i += 1\n",
    "\n",
    "    if ( dist_t[root].prob(word1) > dist_t[root].prob(word2) ):\n",
    "        sentence.append(word1)\n",
    "    else:\n",
    "        sentence.append(word2)\n",
    "\n",
    "    if(order == 2):\n",
    "        while(sentence[i] != \"\" or len(sentence) <= 3):\n",
    "            root = (sentence[i-1], sentence[i])\n",
    "            word1 = get_next(distributions, root, order)\n",
    "            word2 = get_next(distributions, root, order)\n",
    "            i += 1\n",
    "\n",
    "            if ( dist_t[root].prob(word1) > dist_t[root].prob(word2) ):\n",
    "                sentence.append(word1)\n",
    "            else:\n",
    "                sentence.append(word2)\n",
    "            \n",
    "        return sentence\n",
    "\n",
    "    root = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "    word1 = get_next(distributions, root, 3)\n",
    "    word2 = get_next(distributions, root, 3)\n",
    "    i += 1\n",
    "\n",
    "    if ( dist_f[root].prob(word1) > dist_f[root].prob(word2) ):\n",
    "        sentence.append(word1)\n",
    "    else:\n",
    "        sentence.append(word2)\n",
    "\n",
    "    if(order == 3):\n",
    "        while(sentence[i] != \"\" or len(sentence) <= 4):\n",
    "            root = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "            word1 = get_next(distributions, root, order)\n",
    "            word2 = get_next(distributions, root, order)\n",
    "            i += 1\n",
    "\n",
    "            if ( dist_f[root].prob(word1) > dist_f[root].prob(word2) ):\n",
    "                sentence.append(word1)\n",
    "            else:\n",
    "                sentence.append(word2)\n",
    "            \n",
    "        return sentence\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# Edit this one\n",
    "def make_pos_sentence(dist, seed, is_tgram=False):\n",
    "    i=0\n",
    "    sentence = []\n",
    "    sentence.append(seed)\n",
    "    if(not is_tgram):\n",
    "        while(sentence[i] != \"\"):\n",
    "            sentence.append(get_next(dist, sentence[i]))\n",
    "            i += 1\n",
    "    else:\n",
    "        sentence.append(get_next(dist,sentence[i]))\n",
    "        while(sentence[i] != \"\"):\n",
    "            sentence.append(get_next(dist,(sentence[i-1],sentence[i]) ))\n",
    "            i += 1\n",
    "            \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_ngram_probability(distributions, ngram_input, sentence, order=1):\n",
    "    dist_b = distributions[0]\n",
    "    dist_t = distributions[1]\n",
    "    dist_f = distributions[2]\n",
    "\n",
    "    prob = np.log(ngram_input.count(sentence[0])/len(ngram_input))\n",
    "    prob += np.log( dist_b[sentence[0]].prob(sentence[1]) )\n",
    "\n",
    "    if(order == 1):\n",
    "        for i in range(2,len(sentence)):\n",
    "            condition = sentence[i-1]\n",
    "            prob += np.log(dist_b[condition].prob(sentence[i]))\n",
    "            \n",
    "        return prob/len(sentence)\n",
    "    \n",
    "    condition = (sentence[0], sentence[1])\n",
    "    prob += np.log( dist_t[condition].prob(sentence[2]) )\n",
    "\n",
    "    if(order == 2):\n",
    "        for i in range(3,len(sentence)):\n",
    "            condition = (sentence[i-2], sentence[i-1])\n",
    "            prob += np.log(dist_t[condition].prob(sentence[i]))\n",
    "            \n",
    "        return prob/len(sentence)\n",
    "\n",
    "    condition = (sentence[0], sentence[1], sentence[2])\n",
    "    prob += np.log( dist_f[condition].prob(sentence[3]) )\n",
    "\n",
    "    if(order == 3):\n",
    "        for i in range(4,len(sentence)):\n",
    "            condition = (sentence[i-3], sentence[i-2], sentence[i-1])\n",
    "            prob += np.log(dist_f[condition].prob(sentence[i]))\n",
    "            \n",
    "        return prob/len(sentence)\n",
    "\n",
    "def get_pos_probability(dist, pos_input, tag_sentence):\n",
    "    prob = np.log(pos_input.count(tag_sentence[0][0])/len(pos_input))\n",
    "    \n",
    "    for i in range(1,len(tag_sentence)):\n",
    "        prob += np.log(dist[tag_sentence[i-1]].prob(sentence[i]))\n",
    "\n",
    "    return prob/len(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 517943 tokens in the corpus.\n",
      "There are 27075 unique tokens in the corpus.\n",
      "There are 25116 unique tokens that start with a letter.\n",
      "There are 44 unique tags in the corpus.\n"
     ]
    }
   ],
   "source": [
    "# Import and parse the corpus\n",
    "\n",
    "corpus_root = './corpus_clean/'\n",
    "corpus = BracketParseCorpusReader(corpus_root, \".*\")\n",
    "\n",
    "tagged_sentences = corpus.tagged_sents()\n",
    "ngram_input = []\n",
    "pos_input = []\n",
    "legal_tags = [\"EOS\",\"$\",\"#\", \"GW\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\",\"JJR\",\"JJS\",\"LS\",\"MD\",\n",
    "             \"NN\",\"NNS\",\"NNP\",'NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO', \"UH\",'VB',\n",
    "             'VBD',\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\", \"\\\"\", \"\\'\", \",\", \".\", \"AFX\"]\n",
    "\n",
    "single_letter_words = [\"a\", \"i\", \",\", \".\", \"!\", \"?\", \"\\'\", \"\\\"\", \":\", ';', '0', '1', '2', \"3\", '4',\n",
    "                       '5', \"6\", '7', '8', \"9\", \"=\", \"&\", \"#\", '/', '>', \"$\", '<', '+', '%',]\n",
    "\n",
    "# tags_removed = [\"-NONE-\",\"SYM\", \"CODE\", \"ADD\", \"HYPH\",\"-LSB-\", \"-RSB-\",\":\", \"NFP\", \"XX\", \"-LRB-\", \"-RRB-\"]\n",
    "\n",
    "#  Remove -NONE- and  SYM tags from the training data and create a list of tokens and a list of tags.\n",
    "for sentence in tagged_sentences:\n",
    "    for token in sentence:\n",
    "        word = token[0].lower()\n",
    "        tag = token[1]\n",
    "        \n",
    "        if(tag == \"NP\"):\n",
    "            tag = \"NNS\"\n",
    "\n",
    "        if not tag in legal_tags:\n",
    "            del token\n",
    "            continue\n",
    "        \n",
    "        if len(word) == 1:\n",
    "            if not word in single_letter_words:\n",
    "                del token\n",
    "                continue\n",
    "        \n",
    "        if (word[0:5] == \"rsquo\"):\n",
    "            word = \"\\'\" + word[5:]\n",
    "\n",
    "        ngram_input.append(word)\n",
    "        pos_input.append(tag)\n",
    "\n",
    "    ngram_input.append(\"\")\n",
    "    pos_input.append(\"EOS\")\n",
    "\n",
    "unique_alphas = []\n",
    "unique_tokens = list(set(ngram_input))\n",
    "\n",
    "for string in unique_tokens:\n",
    "    if string[0:1].isalpha():\n",
    "        unique_alphas.append(string)\n",
    "\n",
    "print(\"There are\",len(ngram_input),\"tokens in the corpus.\")\n",
    "print(\"There are\",len(unique_tokens),\"unique tokens in the corpus.\")\n",
    "print(\"There are\",len(unique_alphas),\"unique tokens that start with a letter.\")\n",
    "\n",
    "\n",
    "tag_set = set(pos_input)\n",
    "print(\"There are\",len(tag_set),\"unique tags in the corpus.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 517942 bigrams.\n",
      "There are 517941 trigrams.\n",
      "There are 517940 fourgrams.\n"
     ]
    }
   ],
   "source": [
    "# Create bigram and trigram lists\n",
    "bgram = list(ngrams(ngram_input,2))\n",
    "tgram = list(ngrams(ngram_input,3))\n",
    "fgram = list(ngrams(ngram_input,4))\n",
    "\n",
    "pos_bgram = list(ngrams(pos_input,2))\n",
    "\n",
    "\n",
    "# Create conditional frequency distributions\n",
    "cfd_b = ConditionalFreqDist(bgram)\n",
    "\n",
    "cfd_t = ConditionalFreqDist()\n",
    "for trigram in tgram:\n",
    "    condition = (trigram[0], trigram[1])\n",
    "    cfd_t[condition][trigram[2]] += 1\n",
    "\n",
    "cfd_f = ConditionalFreqDist()\n",
    "for fourgram in fgram:\n",
    "    condition = (fourgram[0], fourgram[1], fourgram[2])\n",
    "    cfd_f[condition][fourgram[3]] += 1\n",
    "\n",
    "cfd_pos = ConditionalFreqDist(pos_bgram)\n",
    "\n",
    "cfd_t2w = ConditionalFreqDist()\n",
    "for tag, word in zip(pos_input, ngram_input):\n",
    "    cfd_t2w[tag][word] += 1\n",
    "\n",
    "cfd_w2t = ConditionalFreqDist()\n",
    "for tag, word in zip(pos_input, ngram_input):\n",
    "    cfd_w2t[word][tag] += 1\n",
    "\n",
    "# Create conditional probability distributions\n",
    "cpd_b = ConditionalProbDist(cfd_b, MLEProbDist)\n",
    "cpd_t = ConditionalProbDist(cfd_t, MLEProbDist)\n",
    "cpd_f = ConditionalProbDist(cfd_f, MLEProbDist)\n",
    "\n",
    "cpd_pos = ConditionalProbDist(cfd_pos, MLEProbDist)\n",
    "cpd_t2w = ConditionalProbDist(cfd_t2w, MLEProbDist)\n",
    "cpd_w2t = ConditionalProbDist(cfd_w2t, MLEProbDist)\n",
    "\n",
    "\n",
    "# Consolidate the ngram probability distributions into a single object\n",
    "distributions = [cpd_b, cpd_t, cpd_f]\n",
    "\n",
    "\n",
    "\n",
    "print(\"There are\",len(bgram),\"bigrams.\")\n",
    "print(\"There are\",len(tgram),\"trigrams.\")\n",
    "print(\"There are\",len(fgram),\"fourgrams.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bilbainos' 'cheops' 'swarms' 'interconnection' 'condominium' 'unseen'\n",
      " 'evocative' 'crispin' 'vivamus' 'climbers'] \n",
      "\n",
      "\n",
      " while the strong i motion from measures 90 91 does carry a tonal implication of dominant tonic , such cadential bass motion does not imply endorsement .  \n",
      " -0.520810249251 \n",
      "\n",
      " the park 's 10.5 hectares 25 acres of landscaped gardens and lakes contains a large greenhouse that holds many species of plants , and during a recent cold spell , the wife walked into the living room where her husband was watching television .  \n",
      " -0.413993285015 \n",
      "\n",
      " your gift to goodwill will help us do even more to safeguard the tigers , rhinos , pandas , whales , and other chemicals that are even more adventures ? great paddling destinations from long island to the adirondacks .  \n",
      " -0.514613227829 \n",
      "\n",
      " the principals also focused on pakistan and what it does best helping to identify youth needs , convening groups of individuals and organizations to conserve wildlife and other natural resources and to protect the earth 's surface can significantly modify the gradient of some streams and decreased the gradient of some streams and decreased the gradient of rivers .  \n",
      " -0.388075046283 \n",
      "\n",
      " the best fit of epr species to eq 1 is one equatorial oxygen from two water molecules , one equatorial oxygen from two water molecules , one equatorial oxygen from a hydroxyl group and three equatorial oxygens from carboxyl groups or phosphate .  \n",
      " -0.524899151603 \n",
      "\n",
      " we 've expanded trade adjustment assistance money for you to bring me something to work with companies and associations formed the us wto energy services coalition in late may and has asked the us government gave 245 million dollars to the new york times news service  \n",
      " -0.508651074229 \n",
      "\n",
      " boundaryless characteristics appear more prominently when an industry is knowledge ? rather than capital intensive or when firms operate in economic clusters .  \n",
      " -0.526303483169 \n",
      "\n",
      " the charges were partly offset by a $ 2 million gain on the sale of shares to the mcalpine family interests , for $ 1 million to help state psychological associations develop and lobby for better animal welfare laws at the local levels .  \n",
      " -0.465110138431 \n",
      "\n",
      " his nose was bleeding and his eye looked like it had been charged from its inception to serve as an honest broker is something that congress , having specified 14 years or 28 years , decides that does n't work very well because of the political season .  \n",
      " -0.530132156545 \n",
      "\n",
      " no , we 're the start of combat operations , several hundred cia operatives and special forces soldiers , backed by the striking power of u.s. aircraft and a much larger infrastructure of intelligence and law enforcement communities , was not mood sensitive either .  \n",
      " -0.477017337336 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed_list = np.random.choice(unique_alphas,NUM_SEED)\n",
    "\n",
    "#print(seed_list,\"\\n\\n\")\n",
    "\n",
    "for i in range(NUM_SEED):\n",
    "\n",
    "    sentences = []\n",
    "    full_sentences = []\n",
    "    prob = []\n",
    "\n",
    "    for j in range(PER_SEED):\n",
    "#        sentences.append(make_ngram_sentence(distributions, seed_list[i], 3))\n",
    "        sentences.append(make_ngram_sentence(distributions, \"\", 3))\n",
    "        prob.append(get_ngram_probability(distributions, ngram_input, sentences[j], 3))\n",
    "        full_sentences.append(\" \".join(sentences[j]))\n",
    "\n",
    "    best_index = np.argmax(prob)\n",
    "    print(full_sentences[best_index],\"\\n\",prob[best_index],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '\"', 'not', 'a', 'huge', 'nest', 'that', '\"', 'cooke', 's.', 'city', 'chair', 'which', 'should', 'to', 'well', \"n't\", 'to', 'views', 'came', 'anti-mitochondrial', 'purple', '.', '']\n"
     ]
    }
   ],
   "source": [
    "def get_next_tag(pos_dist, tag):\n",
    "    return pos_dist[tag].generate()\n",
    "\n",
    "def get_next_word(t2w_dist, tag):\n",
    "    return t2w_dist[tag].generate()\n",
    "\n",
    "\n",
    "def make_pos_sentence(pos_dist, t2w_dist, w2t_dist, seed):\n",
    "    i=0\n",
    "    tag = w2t_dist[seed].generate()\n",
    "\n",
    "    sentence = []\n",
    "    sentence.append(\"\")\n",
    "\n",
    "    tags = []\n",
    "    tags.append(\"EOS\")\n",
    "\n",
    "    while(sentence[i] != \"\" or len(sentence) <= 2 ):\n",
    "        tags.append(get_next_tag(pos_dist, tags[i]))\n",
    "        sentence.append(get_next_word(t2w_dist, tags[i]))\n",
    "        i += 1\n",
    "        \n",
    "    return sentence\n",
    "    \n",
    "print(make_pos_sentence(cpd_pos, cpd_t2w, cpd_w2t, \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
